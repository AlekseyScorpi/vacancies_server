import os
from typing import List

from schemas import ModelConfig
from llama_cpp import Llama

class Model:
    """Model class for Saiga-Mistral models
    """
    def __init__(self, config: ModelConfig) -> None:
        """Init Saiga Llama 3 model

        Args:
            model_config (ModelConfig): config for model initialization, more information in ModelConfig class
        """
        model_path = os.path.join(config.folder_name, config.file_name)
        
        self._model = Llama(
            model_path=model_path,
            n_gpu_layers=config.gpu_layers,
            n_threads=config.threads,
            n_ctx=config.context_length,
            n_batch=config.max_new_tokens,
            verbose=False,
        )
        self._system_prompt = config.system_prompt
        
    def _create_prompt(self, request: str) -> str:
        """This function create correctly prompt based on model
        system prompt and user request

        Args:
            request (str): user request to the model

        Returns:
            str: correctly prompt which can be transmitted to the model
        """
        prompt: str = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{self._system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>
{request}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''

        return prompt
    
    def _get_request(
        self,
        vacancy_name: str,
        company_name: str = "",
        company_place: str = "",
        schedule: str = "",
        experience: str = "",
        key_skills: List[str]=[],
    ) -> str:
        """This function preprocess the user's request

        Args:
            vacancy_name (str): Vacancy name
            company_name (str, optional): Company name. Defaults to "".
            company_place (str, optional): Comany place (vacancy place). Defaults to "".
            schedule (str, optional): Vacancy schedule. Defaults to "".
            experience (str, optional): Vacancy experience. Defaults to "".
            key_skills (List[str], optional): List of key skills for this vacancy. Defaults to [].

        Returns:
            str: return user request based on user form from web client
        """
        capitalize_skills: List[str] = [f"'{skill.capitalize()}'" for skill in key_skills]
        
        request: str = (f'Напиши текст вакансии для должности "{vacancy_name}". Название компании: "{company_name}". ',
                   f'Расположение: {company_place}. График работы: {schedule}. Опыт работы: {experience}. ',
                   f'Ключевые навыки: {", ".join(capitalize_skills)}') # type: ignore
        return request

    def generate(
        self,
        vacancy_name: str,
        company_name: str = "",
        company_place: str = "",
        schedule: str = "",
        experience: str = "",
        key_skills: List[str]=[],
    ) -> str:
        """This function pass correct prompt to the model and return model result.

        Args:
            vacancy_name (str): Vacancy name
            company_name (str, optional): Company name. Defaults to "".
            company_place (str, optional): Comany place (vacancy place). Defaults to "".
            schedule (str, optional): Vacancy schedule. Defaults to "".
            experience (str, optional): Vacancy experience. Defaults to "".
            key_skills (List[str], optional): List of key skills for this vacancy. Defaults to [].

        Raises:
            RuntimeError: this exception is raising when unexpected generation error occured

        Returns:
            str: vacancy text generated by LLM
        """
        
        request: str = self._get_request(
            vacancy_name,
            company_name,
            company_place,
            schedule,
            experience,
            key_skills,
        )
        
        prompt: str = self._create_prompt(request=request)
        prompt_bytes = bytes(prompt, encoding='utf-8')
        
        error_counter = -1
        outputs = ""
        
        try:
            while outputs == None or outputs == "" or len(outputs) < 20:
                error_counter += 1
                if error_counter > 2:
                    raise RuntimeError("Unexpected model generation error")
                inputs = self._model.tokenize(prompt_bytes, add_bos=False, special=True)
                result: dict = self._model(
                    inputs, # type: ignore
                    max_tokens=self._model.n_batch,
                    stop=[
                        '<|eot_id|>',
                        '<|eot_|>',
                        '|<|end_of_ text|>',
                        '|<|end_of_text|>',
                        '<|end_of_text|>',
                        '|<|end_header_id|>',
                        '<|eot_1|>',
                        '<|eot_2|>',
                    ]
                )
                outputs = result['choices'][0]['text']
        except:
            raise RuntimeError("Unexpected model generation error")
        
        while ';' in outputs:
            outputs = outputs.replace(';', '\n')
        return outputs